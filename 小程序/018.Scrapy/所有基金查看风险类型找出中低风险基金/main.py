#coding:utf-8
"""
如果用爬虫需要加上假的请求头欺骗
不行的话就使用Selenium方式访问
"""
'''
headers释义：
Accept:客户端可识别的内容类型列表。
Accept-Language: 访问者希望采用的语言或语言组合
Cache-Control:控制缓存
User-Agent:产生请求的浏览器类型
Connection:keep-alive保持长链接，close为不希望使用长链接
Referer: 链接来源
'''
"""
发送请求后，返回各种形式的响应内容：
　　1）r.text：以文本格式返回响应内容
　　2)r.content：以字节格式返回响应内容
　　3)r.json()：以json格式返回相应内容，因为就算请求出错也会返回一串json格式的字符串。所以可以使用r.status_code或者r.raise_for_status来判断响应是否成功
　　4)如果在原始请求中设置了stream=True，可以使用r.raw.read()
"""
import time
import requests
import re
import pymysql
import random

gupiaoxing = "001230,008552,008412,399011,007288,004805,161035,005554,000373,008413,000968,005555,000376,002982,001193,161036,010777,006228,009414,008709,010321,010778,006229,160127,005237,009852,160219,009162,160144,005238,009502,161122,009503,010572,000974,000960,002300,005303,005209,006756,006757,005210,012371,005304,007076,009864,012372,007077,009805,160635,010366,000711,011040,004851,011326,009865,011041,009163,004995,001915,010245,000913,010204,001764,009733,001344,007883,009988,161726,012417,009734,501009,006675,501010,009017,006676,008107,009898,006671,010592,008154,008155,010593,501005,011601,501006,006816,502056,011602,009881,006817,162412,012323,004040,004041,010387,010388,012211,011966,011832,011645,011967,012080,012081,011286,012245,011373,011123,011122,011374,011833,011646,012244,011269,012207,011824,012728,012606,012547,012364,012550,012445,012462,012635,012722,011825,012605,012630,012927,012926,012637,012894,012696,011225,012553,012907,011254,011255,012636,011224,012738,011035,012725,012326,012537,012538,012800,012499,012596,011589,012763,012882,012629,012863,010961,010957,012755,012754,012911,012811,012764,012697,011882,012549,012548,012970,012862,011285,012883,011883,012724,012498,012276,012551,013048,012810,012808,012737,012730,011036,012857,012837,012504,012543,012321,012658,012655,012698,012544,012503,012315,012729,012895,013074,013035,013132,011107,011108,012836,012402,012898,012899,012401,012365,012731,012552,012930,012973,012597,012886,012723,012912,012873,012206,013020,013019,012322,012874,012908,013134,012517,012275,011839,012600,012974,012842,011270,012885,013133,012634,012884,012516,012461,012679,013234,013330,013331,011861,013473,013606,013302,013299,013298,013163,013151,013152,012319,012320,013444,013162,013233,013276,970042,012046,013307,013304,013315,011793,013218,013242,013119,013031,012901,013474,013437,013105,005626,013413,012559,013286,012394,013597,012980,013080,013082,012045,011840,013277,012646,013196,013446,012405,012832,012969,013188,012835,013316,012151,013129,013306,012210,013014,011934,012769,013314,012316,012152,013613,012599,011935,013049,013219,013262,013292,013084,013319,013320,013438,013178,012768,013816,013004,013093,012560,012838,013310,013305,013340,013355,013125,013311,013273,013448,013177,012496,013130,012619,013081,013475,013195,013275,013094,012831,012699,013083,012979,013120,013511,011860,013443,013005,012598,013303,013278,012680,013122,013505,013453,013810,012645,013442,012761,013596,013317,013803,012713,012712,013476,013179,013180,013332,012419,012497,013313,012900,013121,013318,013339,013106,013126,011350,013013,013291,013639,011590,014118,014380,012620,013918,013447,012327,012872,014130,013926,014052,013622,013605,013478,014046,013050,013919,013477,014117,013881,013802,013817,013032,012762,012875,013811,013640,013415,013834,013174,013601,013602,013416,011349,012340,012341,013833,013927,013445"

data = gupiaoxing.split(",")

index = "http://fund.eastmoney.com/"

def getdata(*args, **kwargs):
    from fake_useragent import UserAgent

    ua = UserAgent(verify_ssl = False)
    headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
               'Accept - Encoding' : 'gzip, deflate',
               'Accept-Language': 'zh-CN,zh;q=0.9',
               'Cache-Control': 'max-age=0',
               'User-Agent': ua.random,
               'Connection': 'keep-alive',
               'Referer': 'http://www.baidu.com/'
                #'Content-Type':'application/json',
               }

    for i in data:
        url = index + i + ".html"
        #拼出了整个请求地址
        #print(url)
        try:
            html = requests.get(url = url, headers = headers)
        except Exception as e:
            print (e)
            time.sleep(1200)
            #如果对方检测到我在爬那就停，十分钟
            html = requests.get(url=url, headers=headers)
        html.encoding = 'utf-8'
        html = str(html.text)
        #print (html)
        a = re.search("([无低中高]+风险)", html)
        b = re.search("<title.*?>(.+?)</title>", html)
        #print(a.group())
        print(b.group())

        name = b.group()

        code = re.search("\([0-9]+\)", name)

        fundname, shit = name.split(code.group())
        #先把fundname分离出来再提出code的括号
        code = code.group().strip("(").strip(")")

        fundname = fundname.strip("<title>")


        risk = a.group()
        # print(code)
        # print(fundname)
        # print (risk)

        writeSQL(fundname, code, risk, url)

        randomnum = random.randint(20,60)
        time.sleep(randomnum)

def writeSQL(name, code, risk, url):
    try:
        conn = pymysql.connect(host='127.0.0.1', user='root', passwd='000000', db='stockmain',
                                   charset='utf8')  # 连接数据库
        cur = conn.cursor()  # 使用cursor()方法获取操作游标

        insert = "INSERT INTO stockfundrisk (`fundname`, `fundcode`, `risk`,`url`) VALUES (\'%s\' , \'%s\', \'%s\', \'%s\') ;" % (name, code, risk, url)

        cur.execute(insert)
        # 插入单条数据

    # 将执行的语句记录到日志中
    except Exception as e:
        # 如果插入报错可以在这里用update
        # 因该不用，到时候直接在实时刷价格的时候写update代码，或者说有新字段增加的话需要update
        print(e)

    finally:
        cur.close()
        # 关闭游标
        conn.commit()
        # 没有这个无法真正提交数据
        conn.close()
        # 关闭数据库连接


getdata()

